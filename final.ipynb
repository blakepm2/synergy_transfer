{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synergy Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Blake McBride<br>\n",
    "Email: blakepm2@illinois.edu<br>\n",
    "Course: CS 590 Deep Learning for Healthcare<br>\n",
    "Professor: Jimeng Sun<br>\n",
    "\n",
    "This notebook presents the reproduction results for the paper **\"Anticancer drug synergy prediction in understudied tisues using transfer learning\"** ([Kim et al. 2020](https://doi.org/10.1101/2020.02.05.932657)). All major code components have been reused and adapted from the work of the original authors, with modifications made for reproduction purposes.\n",
    "\n",
    "The original publication can be accessed [here](https://academic.oup.com/jamia/article-abstract/28/1/42/5920819?redirectedFrom=fulltext&login=false).\n",
    "\n",
    "The original publication GitHub repoitory can be accessed [here](https://github.com/yejinjkim/synergy-transfer).\n",
    "\n",
    "The public GitHub repository for this project can be accessed [here](https://github.com/blakepm2/synergy_transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pursuit of enhancing the efficacy of cancer treatments, the exploration of synergistic drug combinations has emerged as a promising strategy. Indeed, Kim et al. demonstrate in their paper how high-throughput combinatorial drug screening offers a pathway to discover these potent combinations, leveraging the complex interplay between multiple drugs to achieve higher efficacy without compromising safety ([Kim et al. 2020](https://doi.org/10.1101/2020.02.05.932657)). Whereas traditional drug discovery processes are not only costly and time-consuming but often result in therapies that target single molecules or pathways, potentially missing opportunities for enhanced therapeutic effects through drug combinations.\n",
    "\n",
    "A significant challenge in this domain is the uneven distribution of data across various cancer types. While some cancers have been extensively studied, providing a rich dataset of drug responses, others, such as bone and prostate cancers, remain understudied due to various obstacles including the difficulty in obtaining viable cell lines and the complexity of their in-vitro models ([Kim et al. 2020](https://doi.org/10.1101/2020.02.05.932657)). This disparity poses a problem for developing effective treatments for these less-studied tissues, as the lack of data hampers the predictive power of drug response models. Addressing this issue, Kim et al. present a novel approach to predicting effective drug combinations in tissues with scant data. By employing **transfer learning**, the study leverages the abundance of data from well-studied tissues to enhance predictions in data-poor tissues, thus overcoming a significant hurdle in cancer treatment research. The methodology centers around the development of a deep neural network model that integrates a comprehensive set of genetic, molecular, and phenotypic features collected from various databases. This model not only predicts drug synergy but also adapts the knowledge learned from data-rich contexts to those that are data-deficient.\n",
    "\n",
    "The core innovation of this study lies in its use of transfer learning, a technique where a model developed for one task is reused as the starting point for a model on a second task. Here, it involves transferring the insights gained from drug-cell interactions in common cancers to improve synergy predictions in less common ones. This approach is particularly advantageous in dealing with the sparse data scenarios typical of understudied tissues. By doing so, the model provides a means to prioritize future in-vitro experiments, potentially accelerating the discovery of viable cancer treatments for these challenging cases.\n",
    "\n",
    "The work of Kim et al. significantly contributes to the field of computational drug discovery by demonstrating how advanced machine learning techniques can bridge the gap between data-rich and data-poor scenarios. This not only enhances the predictive accuracy of drug synergy models but also broadens the scope of their applicability to a wider range of tissues, ultimately facilitating more targeted and effective cancer treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scope of Reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the reproducability of this paper, I encountered several issues while attempting to reproduce the authors' results ([Kim et al. 2020](https://doi.org/10.1101/2020.02.05.932657)). I should make note here that this has been a consistent issue I've faced throughout the course of this project, as this is the 2nd project that I've attempted. To demonstrate the sincere efforts that have been put into this project without being able to showcase any final results, I will detail my process here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Previous Work: MoleculeSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, I chose to reproduce the results of [Chao et al. (2023)](https://doi.org/10.1038/s42256-023-00759-6) for my assignment. In their work, Chao et al. premiered MoleculeSTM, a multi-modal molecule structure-text model for text-based retrieval and editing. The goal of their work was to showcase a new machine learning approach for drug discovery that, in separation from other work in this domain, would jointly learn molecules chemical structures as well as their textual descriptions through a contrastive learning approach ([Chao et al. 2023](https://doi.org/10.1038/s42256-023-00759-6)). I was unable to reproduce the results of this paper for two primary reasons: the complexity of environmental dependencies, and the computational feasibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the environment setup process, this proved to be very taxing and took nearly a week to successfully complete. This seems to stem from a dependency conflict between some of the packages used, namely between PyTorch, CUDA, and Apex. I tried to configure the environment on three different operating systems; starting with Windows 11, then MacOS Sonoma, and finally Ubuntu 22.04. I reached out to ShangChao Liu for support and his advice was to set the environment up on Linux, so I partitioned my hard drive and installed Ubuntu, at which point I was able to get through the environment setup successfully. Additionally, after noticing that others were experiencing similar difficulties with the environment setup, I recieved permission from Dr. Liu to make a pull request to the respository with my fix, which explicitly set a compatible version of CUDA binaries to be installed with the PyTorch installation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second issue that made this paper difficult to reproduce within the scope of this project was the computational feasibility. Using an NVIDIA GTX 4090 GPU, I successfully started the pretraining process on Friday, April 12th, and left it running for 2 weeks. I started having concerns after seeing minimal progress the next morning, so I started researching alternative papers from a collection of backups that I kept. After letting the pretraining process run, I was disheartened to see that only 30/100 epochs had completed. I again reached out to Dr. Liu for guidance and he informed me that he and his colleagues originally trained MoleculeSTM on a computing cluster of multiple machines each outfitted with several CPUs and at least 4 NVIDIA 2080 GPUs. This was not a realistic or feasible paper for me to reproduce in time for submission without spending a significant amount of money on accessing higher-powered computing space. Unfortunately, without realizing how long the pretraining process was estimated to take, I neglected to add code to save checkpoints as it went along, which means that after terminating the process I had nothing to show for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching to Synergy Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding to switch projects to one that would be more feasible required me to examine my backup papers for one that had a lower computational complexity and had readily accessible data, as I did not anticipate a need to request access to any of the MIMIC data given that MoleculeSTM had its data stored in HuggingFace repositories. To this end, I landed on Synergy Transfer by [Kim et al. (2020)](https://doi.org/10.1101/2020.02.05.932657), which seemed promising due to its comparatively lower system requirements and open source data usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absences and Misalignments in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While my shift from MoleculeSTM to Synergy Transfer was done primarily for its use of publicly available data, I quickly found that this was a significant challenge due to the unavailability of some of the original data used and the lack of documentation for acquiring it or replicating their findings. Without documentation of any kind, I followed the only indications I had from the original paper as well as the GitHub repo, which stated that data had been accessed from DrugComb and DrugBank. It took time to understand how to acquire the data using the API from DrugComb, but once I did manage to get the data I noticed that there had been significant changes to the data architecture since this paper was firts published. Specifically, key dimensions reflecting the tissue and disease names, as well as the synergy scores for various blocks were absent from the new version of data available on DrugComb. I refactored the code as best as I could to align with the new dimensionality, but there were still issues with the absence of tissue and disease names that caused downsteam errors in the Synergy Prediction task, as will be seen in future sections. \n",
    "\n",
    "Where DrugBank was concerned, I had no realistic pathway to help me acquire it, so I similarly decided to reach out to two of the original authors of the paper, Kim Yejin and Jing Tang, for guidance. Kim Yejin shared with me a .zip file containing some of the data used for downstream tasks in the preprocessing notebook, but it was not inclusive of all of the original data they used. When I requested all of the original data used, she did not respond. Jing Tang shared the original summary .csv file that I was looking for, but the dimensionality of this data did not match the newer version or the version that they used in the original paper, so I was still blocked from fully realizing their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without access to the original data used for this paper, I was unable to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover the methodology used for reproducing the results of [Kim et al. (2020)](https://doi.org/10.1101/2020.02.05.932657), including a full setup guide for the environment, an overview of available data, the model architecture, training process, and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning the environment setup, I should note that I used a machine with Ubuntu v22.04 installed and would recommend doing the same to avoid some complexity with package installation. Assuming you're working on some distribution of Linux or want to attempt the setup with a different OS, the first step is to install Conda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install Conda as shown below, you can refer to more comprehensive instructions on [Conda's website](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) if you're using a different OS.\n",
    "\n",
    "```\n",
    "mkdir -p ~/miniconda3\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\n",
    "bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\n",
    "rm -rf ~/miniconda3/miniconda.sh\n",
    "```\n",
    "\n",
    "Then initialize bash and zsh shells\n",
    "\n",
    "```\n",
    "~/miniconda3/bin/conda init bash\n",
    "~/miniconda3/bin/conda init zsh\n",
    "```\n",
    "\n",
    "Create a new conda environment using Python 3.8 as the base. To make things easier, you can use the following command to create a new conda environment using the [environment.yaml](environment.yaml) file, as shown below:\n",
    "\n",
    "```\n",
    "conda env create -f environment.yaml\n",
    "```\n",
    "\n",
    "Or, if you wish to do it yourself, create your new conda environment as shown below:\n",
    "\n",
    "```\n",
    "conda create -n synergy python=3.8\n",
    "```\n",
    "\n",
    "and activate it\n",
    "\n",
    "```\n",
    "conda activate synergy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a conda environment successfully configured, you now must install all of the required Python dependencies.\n",
    "\n",
    "```\n",
    "pip install torch torchvision\n",
    "pip install cudatoolkit==12.1\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install seaborn\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "pip install rdkit\n",
    "```\n",
    "\n",
    "At this point you should be all set up to start running the code in this notebook. Ensure you select the conda environment you created as the Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the environment has been successfully configured, the next step is accessing and preprocessing the data. The section below provides download instructions for all of the data used for this project (both raw and processed). You can run the code below to perform all preprocessing steps manually, or you can simply use the provided processed data used for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "import re\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import math\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "from utilities import Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your data path as the directory in which you saved the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'downloaded_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to download all of the data to your data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 1tJjbItceWewAFKT6GbkiEc0uetG9v66Y cell\n",
      "Retrieving folder 1McFcm50uxfOhSLdV32mpwi1y64WEbyLl processed\n",
      "Processing file 1wn6X6CKzBorf74qYfj299kGjcQh6qcC- cell_features.p\n",
      "Retrieving folder 1DFpAvjQkXsSeR-AQwQZJ_ZQWXMh7Udfn raw\n",
      "Processing file 1vHCitsb1wwjZpb5d5Oph6zefuOImINP1 cell_line_meta.csv\n",
      "Processing file 1WEhqtuSDjZgPAqFBdcYY8d1-pUqY_96s cell_lines.csv\n",
      "Retrieving folder 1NXmg7tbKZP2anOXrJ4dhDbIToFiHUDrx drug\n",
      "Retrieving folder 1uZutTWVUKDwYlkBQTDvCTwRnIKOQeO4s processed\n",
      "Processing file 1OQYj57LwSqJgRXNLHwCeLIo_4E_6g9kW drug_clean_fp.p\n",
      "Processing file 1TZ1v9l4GIQO8fchQPJFEpZHs-Q0LvaP9 drug_clean.csv\n",
      "Processing file 1ZO-Tzwxz1lUYU8RuhWu3G5ZHYwI32cQm drug_features.p\n",
      "Retrieving folder 1DX9dWJPOq3IzyELCH719FXGN92Rjddah raw\n",
      "Processing file 18mZ8ywSYDuUNgeG03YSpznvwdeA68xcy drug_DrugBank_target.csv\n",
      "Processing file 1rIs8Mlm9BJQCyWI1l0QT9h9bn4AdZvsc drug_lincs_target.csv\n",
      "Processing file 1ItbIrk0bpqGEIdfv1S_DC8wUg4I6tasX drug_TTD_target.csv\n",
      "Processing file 1BwIuZb2JhBs07lPOEtg5fRfbjldXBRGB drugs.csv\n",
      "Retrieving folder 14SpivIgJRrLnlHn1QYIgcGd_flPaB1J9 gene\n",
      "Retrieving folder 1OZk6L6URo70u5XOfjAjteSFTkXjzAu4p raw\n",
      "Processing file 1P2EJU5QoLz5X0ChiYwsIeDPLUYCaCWpn fpkm_broad.csv\n",
      "Processing file 1cD6s-mEoaSLzF6kSkIkYhlpoAKY430sC fpkm_sangar.csv\n",
      "Processing file 1euEjVnSsYl_FC32NxwHaxzuuvh0pL7BM gene_identifiers_latest.csv\n",
      "Retrieving folder 1trZ5MTycQYS30WH2krNktqeAfPSLDKVf models\n",
      "Processing file 1d2MOuFBIMffUHd4rOAwhvuVlC_ncHMYA cellGeneCompressed.p\n",
      "Processing file 1YGf5KJr4DmAIbWpzdTW39XMeObdwdtLd cellGeneCompressor.p\n",
      "Processing file 1KpkX68hk7wOLZ2pdYy76eE9dvV1TSAZ0 drugGeneCompressed.p\n",
      "Processing file 1JbkxIOVFvd1LJOQr8bZ6yFzobguKteM6 drugGeneCompressor.p\n",
      "Processing file 1bHutzJt4Rh9SiJTkwYA5SYmbotn34-P1 id_feat_gene.p\n",
      "Retrieving folder 18nsrwRTRk6OAgwXTZrfydsoAlTEsif8U summary\n",
      "Retrieving folder 1Q6dGgNAgQsSpUh3JI6tYnj0-x1MnFs7x processed\n",
      "Processing file 1JgoRl74VF7-53lSAMoHgKMc8wozBZAtj codes.p\n",
      "Processing file 1BC18VDC2Zt9meWrqpw37SlXJNWceAFQN summary_mean.p\n",
      "Retrieving folder 1lUoGN9JQs0e1_y-JbJkPqxTwHr9SJvIs raw\n",
      "Processing file 158U6k-BN-ANkufnFejjDUBNQO9m8oymR summary_table.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1wn6X6CKzBorf74qYfj299kGjcQh6qcC-\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/cell/processed/cell_features.p\n",
      "100%|██████████| 43.7M/43.7M [00:01<00:00, 30.6MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vHCitsb1wwjZpb5d5Oph6zefuOImINP1\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/cell/raw/cell_line_meta.csv\n",
      "100%|██████████| 256k/256k [00:00<00:00, 6.99MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WEhqtuSDjZgPAqFBdcYY8d1-pUqY_96s\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/cell/raw/cell_lines.csv\n",
      "100%|██████████| 259k/259k [00:00<00:00, 6.90MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OQYj57LwSqJgRXNLHwCeLIo_4E_6g9kW\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/processed/drug_clean_fp.p\n",
      "100%|██████████| 2.29M/2.29M [00:00<00:00, 23.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TZ1v9l4GIQO8fchQPJFEpZHs-Q0LvaP9\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/processed/drug_clean.csv\n",
      "100%|██████████| 762k/762k [00:00<00:00, 14.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZO-Tzwxz1lUYU8RuhWu3G5ZHYwI32cQm\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/processed/drug_features.p\n",
      "100%|██████████| 1.98M/1.98M [00:00<00:00, 25.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18mZ8ywSYDuUNgeG03YSpznvwdeA68xcy\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/raw/drug_DrugBank_target.csv\n",
      "100%|██████████| 4.22M/4.22M [00:00<00:00, 20.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rIs8Mlm9BJQCyWI1l0QT9h9bn4AdZvsc\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/raw/drug_lincs_target.csv\n",
      "100%|██████████| 787k/787k [00:00<00:00, 15.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ItbIrk0bpqGEIdfv1S_DC8wUg4I6tasX\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/raw/drug_TTD_target.csv\n",
      "100%|██████████| 296k/296k [00:00<00:00, 7.57MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1BwIuZb2JhBs07lPOEtg5fRfbjldXBRGB\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/drug/raw/drugs.csv\n",
      "100%|██████████| 734k/734k [00:00<00:00, 13.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1P2EJU5QoLz5X0ChiYwsIeDPLUYCaCWpn\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/gene/raw/fpkm_broad.csv\n",
      "100%|██████████| 56.0M/56.0M [00:02<00:00, 26.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1cD6s-mEoaSLzF6kSkIkYhlpoAKY430sC\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/gene/raw/fpkm_sangar.csv\n",
      "100%|██████████| 33.7M/33.7M [00:01<00:00, 30.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1euEjVnSsYl_FC32NxwHaxzuuvh0pL7BM\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/gene/raw/gene_identifiers_latest.csv\n",
      "100%|██████████| 2.91M/2.91M [00:00<00:00, 27.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1d2MOuFBIMffUHd4rOAwhvuVlC_ncHMYA\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/models/cellGeneCompressed.p\n",
      "100%|██████████| 57.5k/57.5k [00:00<00:00, 3.53MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YGf5KJr4DmAIbWpzdTW39XMeObdwdtLd\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/models/cellGeneCompressor.p\n",
      "100%|██████████| 25.0M/25.0M [00:00<00:00, 25.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KpkX68hk7wOLZ2pdYy76eE9dvV1TSAZ0\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/models/drugGeneCompressed.p\n",
      "100%|██████████| 1.06M/1.06M [00:00<00:00, 18.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1JbkxIOVFvd1LJOQr8bZ6yFzobguKteM6\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/models/drugGeneCompressor.p\n",
      "100%|██████████| 12.6M/12.6M [00:00<00:00, 29.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1bHutzJt4Rh9SiJTkwYA5SYmbotn34-P1\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/models/id_feat_gene.p\n",
      "100%|██████████| 2.22M/2.22M [00:00<00:00, 26.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1JgoRl74VF7-53lSAMoHgKMc8wozBZAtj\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/summary/processed/codes.p\n",
      "100%|██████████| 512k/512k [00:00<00:00, 11.3MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1BC18VDC2Zt9meWrqpw37SlXJNWceAFQN\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/summary/processed/summary_mean.p\n",
      "100%|██████████| 21.9M/21.9M [00:00<00:00, 28.8MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=158U6k-BN-ANkufnFejjDUBNQO9m8oymR\n",
      "From (redirected): https://drive.google.com/uc?id=158U6k-BN-ANkufnFejjDUBNQO9m8oymR&confirm=t&uuid=8a09c392-ceb2-4a2e-91b7-37c3259e0fef\n",
      "To: /home/blake/synergy_transfer/downloaded_data/synergy_transfer_data/summary/raw/summary_table.csv\n",
      "100%|██████████| 166M/166M [00:06<00:00, 26.7MB/s] \n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['downloaded_data/synergy_transfer_data/cell/processed/cell_features.p',\n",
       " 'downloaded_data/synergy_transfer_data/cell/raw/cell_line_meta.csv',\n",
       " 'downloaded_data/synergy_transfer_data/cell/raw/cell_lines.csv',\n",
       " 'downloaded_data/synergy_transfer_data/drug/processed/drug_clean_fp.p',\n",
       " 'downloaded_data/synergy_transfer_data/drug/processed/drug_clean.csv',\n",
       " 'downloaded_data/synergy_transfer_data/drug/processed/drug_features.p',\n",
       " 'downloaded_data/synergy_transfer_data/drug/raw/drug_DrugBank_target.csv',\n",
       " 'downloaded_data/synergy_transfer_data/drug/raw/drug_lincs_target.csv',\n",
       " 'downloaded_data/synergy_transfer_data/drug/raw/drug_TTD_target.csv',\n",
       " 'downloaded_data/synergy_transfer_data/drug/raw/drugs.csv',\n",
       " 'downloaded_data/synergy_transfer_data/gene/raw/fpkm_broad.csv',\n",
       " 'downloaded_data/synergy_transfer_data/gene/raw/fpkm_sangar.csv',\n",
       " 'downloaded_data/synergy_transfer_data/gene/raw/gene_identifiers_latest.csv',\n",
       " 'downloaded_data/synergy_transfer_data/models/cellGeneCompressed.p',\n",
       " 'downloaded_data/synergy_transfer_data/models/cellGeneCompressor.p',\n",
       " 'downloaded_data/synergy_transfer_data/models/drugGeneCompressed.p',\n",
       " 'downloaded_data/synergy_transfer_data/models/drugGeneCompressor.p',\n",
       " 'downloaded_data/synergy_transfer_data/models/id_feat_gene.p',\n",
       " 'downloaded_data/synergy_transfer_data/summary/processed/codes.p',\n",
       " 'downloaded_data/synergy_transfer_data/summary/processed/summary_mean.p',\n",
       " 'downloaded_data/synergy_transfer_data/summary/raw/summary_table.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download_folder('https://drive.google.com/drive/u/1/folders/14xZ4BLGMEO0Wv5uY0R2tMtB13hevjVLF', output=DATA_PATH, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset we use is the `summary_table` dataset. This dataset serves as a source for drug and cell_lines information, as well as ids corresponding to various healthcare data sources for joining this information with supplementary data, as we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load in the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'downloaded_data//summary/raw/summary_table.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/summary/raw/summary_table.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/synergy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/synergy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/synergy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/synergy/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/synergy/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloaded_data//summary/raw/summary_table.csv'"
     ]
    }
   ],
   "source": [
    "summary = pd.read_csv(f'{DATA_PATH}/summary/raw/summary_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the average of replicates experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean = summary.groupby(['drug_row','drug_col','cell_line_name', 'study_id'], \n",
    "                               as_index=False).mean(numeric_only=True)\n",
    "\n",
    "summary_mean = summary_mean.loc[:, ['drug_row', 'drug_col', 'cell_line_name', 'study_id','ri_row', 'ri_col', 'synergy_loewe']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the Monotherapy Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = pd.concat([\n",
    "          summary_mean.loc[:, ['drug_row', 'cell_line_name', 'ri_row']].rename(columns={'drug_row':'drug', 'ri_row':'ri'}),\n",
    "          summary_mean.loc[:, ['drug_col', 'cell_line_name', 'ri_col']].rename(columns={'drug_col':'drug', 'ri_col':'ri'})\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Relative Inhibition vs Loewe Synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sensitivity['ri'], bins=20, range=[-100,100])\n",
    "plt.xlabel(\"Relative Inhibition (Monotherapy Sensitivity)\")\n",
    "plt.ylabel(\"Counts\")\n",
    "#plt.savefig(data_path+'ri_cnt.png',  bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(((summary_mean.loc[:,'synergy_loewe'].values.reshape(-1))), range=[-100, 75])\n",
    "plt.xlabel(\"Loewe Synergy\")\n",
    "plt.ylabel(\"Counts\")\n",
    "#plt.savefig(data_path+'syn_cnt.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = summary_mean.loc[:,'ri_row'].to_numpy()\n",
    "Y = summary_mean.loc[:,'ri_col'].to_numpy()\n",
    "Z = summary_mean.loc[:,'synergy_loewe'].to_numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_trisurf(X,Y,Z)\n",
    "\n",
    "ax.set_xlabel('Drug1 Relative Inhibition')\n",
    "ax.set_ylabel('Drug2 Relative Inhibition')\n",
    "ax.set_zlabel('Synergy')\n",
    "ax.set_xlim(-100, 100)\n",
    "ax.set_ylim(-100, 100)\n",
    "\n",
    "# fig.savefig(data_path+'ri_synergy_3d.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming Items to ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leverage the `Mapping` class to map items to indices and update the datasets to ensure consistency across all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = {\n",
    "    'drugs': Mapping(set(summary_mean['drug_row'].unique()).union(set(summary_mean['drug_col'].unique()))),\n",
    "    'cell':Mapping(summary_mean['cell_line_name'].unique())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean['drug_col'] = summary_mean['drug_col'].apply(lambda x: codes['drugs'].item2idx[x])\n",
    "summary_mean['drug_row'] = summary_mean['drug_row'].apply(lambda x: codes['drugs'].item2idx[x])\n",
    "summary_mean['cell_line_name'] = summary_mean['cell_line_name'].apply(lambda x: codes['cell'].item2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the updated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean.to_pickle(f'{DATA_PATH}/summary/processed/summary_mean.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the data for `drugs`. This data includes various drug features and molecular structures to learn the model and help it make synergy predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.read_csv(f'{DATA_PATH}/drug/raw/drugs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our mapping and perform some error correction for certain smiles in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['id']=drug['dname'].apply(lambda x: codes['drugs'].item2idx.get(x))\n",
    "drug = drug.loc[~drug['id'].isna(),:]\n",
    "\n",
    "drug.loc[drug['cid']==57519530,'smiles']='CC(C)CC1C(=O)NC(C(=O)NC(CCCC=CCCCCCCC(C(=O)NC(C(=O)NC(C(=O)NC(C(=O)NC(C(=O)N1)CCCNC(=N)N)CC2=CNC3=CC=CC=C32)CC(C)C)CC(=O)N)(C)NC(=O)C(CC4=CC=CC=C4)NC(=O)C(C(C)O)NC(=O)C(CCC(=O)N)NC(=O)C(CCC(=O)N)NC(=O)C(CO)NC(=O)C(CCC(=O)N)NC(=O)CCNC(=O)C)(C)C(=O)NC(CCC(=O)N)C(=O)NC(CC(=O)N)C(=O)N)CC(C)C'\n",
    "drug.loc[drug['cid']==73265323,'smiles']='CC(C)CCCCCCCC(=O)NC1C(C(C(OC1OC2=C3C=C4C=C2OC5=C(C=C(C=C5)C(C6C(=O)NC(C7=C(C(=CC(=C7)O)OC8C(C(C(C(O8)CO)O)O)O)C9=C(C=CC(=C9)C(C(=O)N6)NC(=O)C4NC(=O)C1C2=CC(=CC(=C2)OC2=C(C=CC(=C2)C(C(=O)NC(CC2=CC(=C(O3)C=C2)Cl)C(=O)N1)N)O)O)O)C(=O)O)OC1C(C(C(C(O1)CO)O)O)NC(=O)C)Cl)CO)O)O'\n",
    "drug.loc[drug['cid']==16131923,'smiles']='CCCCCCCCCC(=O)NC1C(C(C(OC1OC2=C3C=C4C=C2OC5=C(C=C(C=C5)C(C6C(=O)NC(C7=C(C(=CC(=C7)O)OC8C(C(C(C(O8)CO)O)O)O)C9=C(C=CC(=C9)C(C(=O)N6)NC(=O)C4NC(=O)C1C2=CC(=CC(=C2)OC2=C(C=CC(=C2)C(C(=O)NC(CC2=CC(=C(O3)C=C2)Cl)C(=O)N1)N)O)O)O)C(=O)O)OC1C(C(C(C(O1)CO)O)O)NC(=O)C)Cl)CO)O)O'\n",
    "\n",
    "drug.to_csv(f'{DATA_PATH}/drug/processed/drug_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the MAACS fingerprint from the smiles for numeric representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.read_csv(f'{DATA_PATH}/drug/processed/drug_clean.csv')\n",
    "drug['fps']=drug['smiles'].apply(lambda x: list(MACCSkeys.GenMACCSKeys(Chem.MolFromSmiles(x))) if Chem.MolFromSmiles(x) is not None else '')\n",
    "\n",
    "drug.to_pickle(f'{DATA_PATH}/drug/processed/drug_clean_fp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pickle.load(open(f'{DATA_PATH}/drug/processed/drug_clean_fp.p', 'rb'))\n",
    "drug['id'] = drug['id'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we convert the SMILES sequences to indices for zero embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = drug['smiles'].to_list()\n",
    "chars = set([char for seq in seqs for char in seq])\n",
    "chars = ['']+list(chars) #for zero embedding\n",
    "codes['mole'] = Mapping(chars)\n",
    "drug['smiles'] = drug['smiles'].apply(lambda x : [codes['mole'].item2idx[char] for char in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aligning Drug's Target Gene(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the drugs data, we need to align the drugs with their target genes from DrugBank, TTD, and LINC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge target genes from DrugBank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_DrugBank_target = pd.read_csv(f'{DATA_PATH}/drug/raw/drug_DrugBank_target.csv')\n",
    "drug_DrugBank_target =  drug_DrugBank_target[['Gene', 'drugbank_id']].groupby('drugbank_id').agg(lambda x: set(x)).applymap(list).reset_index()\n",
    "drug = pd.merge(drug, drug_DrugBank_target, how='left', on='drugbank_id')\n",
    "drug.rename(columns={'Gene':'gene_drugbank'}, inplace=True)\n",
    "drug['gene_drugbank'] = drug['gene_drugbank'].apply(lambda x: [] if type(x) is float else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge target genes from TTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_TTD_target=pd.read_csv(f'{DATA_PATH}/drug/raw/drug_TTD_target.csv')\n",
    "drug_TTD_target['TTD_TARGETS']=drug_TTD_target['TTD_TARGETS'].apply(lambda x: re.split(',|;', x))\n",
    "drug=pd.merge(drug, drug_TTD_target.loc[:,['TTD_TARGETS', 'cid']],how='left', on='cid')\n",
    "drug.rename(columns={'TTD_TARGETS':'gene_ttd'}, inplace=True)\n",
    "drug['gene_ttd']=drug['gene_ttd'].apply(lambda x: [] if type(x) is float else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge target genes from LINC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_lincs_target = pd.read_csv(f'{DATA_PATH}/drug/raw/drug_lincs_target.csv')\n",
    "drug_lincs_target['target_genes']=drug_lincs_target['target_genes'].apply(lambda x: x.split(',') if type(x) is not float else [])\n",
    "drug = pd.merge(drug, drug_lincs_target.loc[:, ['target_genes', 'cid']], how='left', on='cid')\n",
    "drug.rename(columns={'target_genes':'gene_linc'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we combine all of the target genes identified for a particular drug into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['gene'] = drug.apply(lambda row: list(set(row['gene_drugbank']+row['gene_ttd']+row['gene_linc'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And map the genes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes_drug = [l for lst in drug['gene'].aggregate(list).values for l in lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line = pd.read_csv(f'{DATA_PATH}/cell/raw/cell_lines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct errors in raw data and apply mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line.replace({'large_intestine':'colon'}, inplace=True)\n",
    "cell_line.drop('id',axis=1, inplace=True)\n",
    "cell_line['cell_id'] = cell_line['name'].apply(lambda x: codes['cell'].item2idx.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a subset of cell line features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_tissue = cell_line.loc[~cell_line['cell_id'].isna(), ['cell_id', 'tissue_name', 'disease_name']]\n",
    "cell_line_tissue['cell_id'] = cell_line_tissue['cell_id'].astype(int)\n",
    "cell_line_tissue = cell_line_tissue.groupby('cell_id').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply our mapping logic again to map tissue ids to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['tissue'] = Mapping(cell_line_tissue['tissue_name'].unique())\n",
    "cell_line_tissue['tissue_name'] = cell_line_tissue['tissue_name'].apply(lambda x: codes['tissue'].item2idx[x])\n",
    "cell_line_tissue.rename(columns={'tissue_name':'tissue_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same mapping process for diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['disease'] = Mapping(cell_line_tissue['disease_name'].apply(lambda x: x.split(' ')[-1].lower() if type(x) is not float else '').unique())\n",
    "cell_line_tissue['disease_name'] = cell_line_tissue['disease_name'].apply(lambda x: codes['disease'].item2idx[x.split(' ')[-1].lower()] if type(x) is not float else codes['disease'].item2idx[''])\n",
    "cell_line_tissue.rename(columns={'disease_name':'disease_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform some more error correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['tissue'].item2idx['haematopoietic_and_lymphoid'] = 1\n",
    "codes['tissue'].idx2item[1] = 'hem&lymp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check out what our mapping logic has done to our tissues and diseases for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['tissue'].item2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['disease'].item2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of cell lines per tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_cell_cnts = cell_line_tissue.groupby('tissue_id').count().sort_values(by='disease_id', ascending=False)['disease_id']\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar([codes['tissue'].idx2item[tissue_id] for tissue_id in tissue_cell_cnts.index], tissue_cell_cnts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Tissue')\n",
    "plt.ylabel('Number of Cell Lines')\n",
    "\n",
    "#plt.savefig(data_path+'tissue_num_cell.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aligning Cell Lines with Gene Expression(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the `gene_identifiers` and `cell_line_meta` files, we can align the cell lines with their respective gene expressions from Broad Institute and SANGAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_identifiers = pd.read_csv(f'{DATA_PATH}/gene/raw/gene_identifiers_latest.csv')\n",
    "cell_line_meta = pd.read_csv(f'{DATA_PATH}/cell/raw/cell_line_meta.csv')\n",
    "fpkm_broad = pd.read_csv(f'{DATA_PATH}/gene/raw/fpkm_broad.csv')\n",
    "fpkm_sangar = pd.read_csv(f'{DATA_PATH}/gene/raw/fpkm_sangar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the gene ids to the cell lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_codes={}\n",
    "_codes['gene_id2hgnc_symbol'] = gene_identifiers[['gene_id', 'hgnc_symbol']].set_index('gene_id').to_dict('index')\n",
    "_codes['SANGAR_ID2cell_name'] = cell_line_meta.loc[cell_line_meta['name'].isin(codes['cell'].idx2item) & ~cell_line_meta['SANGAR_ID'].isna(), ['SANGAR_ID', 'name']].set_index('SANGAR_ID').to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align the gene expressions from Broad Institute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpkm_broad = fpkm_broad.loc[fpkm_broad['model_id'].isin(_codes['SANGAR_ID2cell_name'])]#apply(lambda x: _codes['SANGAR_ID2cell_name'].get(x))\n",
    "fpkm_broad['model_id'] = fpkm_broad['model_id'].apply(lambda x: codes['cell'].item2idx[_codes['SANGAR_ID2cell_name'][x]['name']])\n",
    "fpkm_broad.rename(columns={col:_codes['gene_id2hgnc_symbol'][col]['hgnc_symbol'] for col in fpkm_broad.columns[1:]}, inplace=True)\n",
    "\n",
    "fpkm_broad.rename(columns={'model_id':'cell_id'}, inplace=True)\n",
    "fpkm_broad.set_index('cell_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fpkm_broad.sum()>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align gene expressions from SANGAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpkm_sangar = pd.read_csv(f'{DATA_PATH}/gene/raw/fpkm_sangar.csv')\n",
    "\n",
    "fpkm_sangar = fpkm_sangar.loc[fpkm_sangar['model_id'].isin(_codes['SANGAR_ID2cell_name'])]#apply(lambda x: _codes['SANGAR_ID2cell_name'].get(x))\n",
    "fpkm_sangar['model_id'] = fpkm_sangar['model_id'].apply(lambda x: codes['cell'].item2idx[_codes['SANGAR_ID2cell_name'][x]['name']])\n",
    "fpkm_sangar.rename(columns={col:_codes['gene_id2hgnc_symbol'][col]['hgnc_symbol'] for col in fpkm_sangar.columns[1:]}, inplace=True)\n",
    "\n",
    "fpkm_sangar.rename(columns={'model_id':'cell_id'}, inplace=True)\n",
    "fpkm_sangar.set_index('cell_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fpkm_sangar.sum()>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, integrate gene expressions from Broad Institute and SANGAR into the cell lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_gene_fpkm = pd.concat([fpkm_broad,fpkm_sangar], sort=True) \n",
    "cell_gene_fpkm = cell_gene_fpkm.loc[:,(cell_gene_fpkm.std(skipna=True)!=0)] #drop genes with std=0\n",
    "cell_gene_fpkm.fillna(0, inplace=True)\n",
    "\n",
    "cell_gene_fpkm_norm = ((cell_gene_fpkm-cell_gene_fpkm.mean())/cell_gene_fpkm.std()) #z-score, gene-wise\n",
    "\n",
    "unique_genes_cell = cell_gene_fpkm_norm.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the unique genes across all cell lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_genes_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gene Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the unique genes for the cell lines, we can map them to indices in our codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes['gene'] = Mapping(set(unique_genes_cell).union(set(unique_genes_drug)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again count the number of unique genes we have mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(codes['gene'].idx2item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we map the genes back to our drugs data and extract a subset of drug information to serve as our features for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['gene_id']=drug['gene'].apply(lambda genes: [codes['gene'].item2idx[gene] for gene in genes])\n",
    "drug_features = drug[['id', 'smiles', 'fps', 'gene_id']].groupby('id').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the drug features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_features.to_pickle(f'{DATA_PATH}/drug/processed/drug_features.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge the cell line's tissue and disease type with the gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_gene = cell_gene_fpkm_norm.apply(lambda row: {codes['gene'].item2idx[gene]:row[gene] for gene in cell_gene_fpkm_norm.columns}, axis=1).reset_index()\n",
    "cell_gene.rename(columns={0:'gene_id'}, inplace=True)\n",
    "cell_gene = cell_gene.groupby('cell_id').first()\n",
    "cell_line = pd.merge(cell_line_tissue, cell_gene, how='left', on='cell_id')\n",
    "cell_line['gene_id']=cell_line['gene_id'].apply(lambda x: {} if type(x) is float else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the updated cell lines and our code map for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line.to_pickle(f'{DATA_PATH}/cell/processed/cell_features.p')\n",
    "pickle.dump(codes, open(f'{DATA_PATH}/summary/processed/codes.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data exploration and visualization purposes, we will add the cell line's tissue information into our summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mean['tissue'] = summary_mean['cell_line_name'].apply(lambda x: cell_line_tissue.loc[x, 'tissue_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of blocks per cell line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_exp = summary_mean.groupby('cell_line_name').count()['drug_row'].sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.bar([codes['cell'].idx2item[cell_id]\n",
    "         +'('+codes['tissue'].idx2item[cell_line_tissue.loc[cell_id, 'tissue_id']]\n",
    "         +')' for cell_id in cell_line_exp.index], cell_line_exp)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('cell line')\n",
    "plt.ylabel('number of blocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of blocks per tissue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_exp = summary_mean.groupby('tissue').count()['drug_row'].sort_values(ascending=False)\n",
    "\n",
    "plt.bar([codes['tissue'].idx2item[tissue_id] for tissue_id in tissue_exp.index], tissue_exp)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('tissue')\n",
    "plt.ylabel('number of drug combinations')\n",
    "#plt.savefig(data_path+'tissue_num_comb.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of blocks per study & tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_ids = ['', 'ONEIL','CLOUD','ALMANAC','FORCINA','NCATS_ATL','Mathews','NCATS_DIPG','NCATS_ES(FAKi/AURKi)','NCATS_ES(Nampt+PARP)','Wilson','NCATS_HL','Yohe','NCATS_2D_3D','Phelan','NCATS_MDR_CS','CCLE','CTRPv2','FIMM','gCSI','GDSC1','GRAY','UHNBreast','BEATAML'] #from study.csv\n",
    "num_blocks = summary_mean.groupby(['study_id', 'tissue'], as_index=False)['drug_row'].count().rename(columns={'drug_row':'num_blocks'})\n",
    "num_blocks['tissue'] = num_blocks['tissue'].apply(lambda x: codes['tissue'].idx2item[x])\n",
    "num_blocks['study_id'] = num_blocks['study_id'].apply(lambda x: study_ids[int(x)])\n",
    "\n",
    "# num_blocks_pivot=num_blocks.pivot(\"study_id\", \"tissue\", \"num_blocks\").fillna(0)\n",
    "num_blocks_pivot = num_blocks.pivot(index=\"study_id\", columns=\"tissue\", values=\"num_blocks\").fillna(0)\n",
    "num_blocks_pivot = num_blocks_pivot.loc[['ALMANAC', 'CLOUD', 'NCATS_HL', 'FORCINA','Mathews','Wilson','Yohe','NCATS_MDR_CS','Phelan','NCATS_ATL','NCATS_2D_3D',\n",
    "                      'NCATS_DIPG', 'NCATS_ES(FAKi/AURKi)',\n",
    "                      'NCATS_ES(Nampt+PARP)','ONEIL'],\n",
    "                     ['brain','breast','colon','endometrium','hem&lymp','kidney','lung','ovary',\n",
    "                      'bone','prostate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm = LogNorm(vmin=num_blocks_pivot.min().min()+1, vmax=num_blocks_pivot.max().max())\n",
    "cbar_ticks = [math.pow(10, i) for i in range(math.floor(math.log10(num_blocks_pivot.min().min()+1)), 1+math.ceil(math.log10(num_blocks_pivot.max().max())))]\n",
    "\n",
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "\n",
    "ax = sns.heatmap(num_blocks_pivot.replace(0,1),\n",
    "           norm=log_norm,\n",
    "            cbar_kws={'ticks':cbar_ticks},\n",
    "            cmap=cmap)\n",
    "\n",
    "#fig=ax.get_figure()\n",
    "#fig.tight_layout()\n",
    "#fig.savefig(data_path+'study_heatmap.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts number of unique drugs for each tissue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_drugs = pd.merge(summary_mean.groupby('tissue')['drug_row'].apply(set).reset_index(name='drug_row'),\n",
    "         summary_mean.groupby('tissue')['drug_col'].apply(set).reset_index(name='drug_row'),\n",
    "         on='tissue')\n",
    "tissue_drugs['drugs']=tissue_drugs.apply(lambda row: row[1].union(row[2]), axis=1)\n",
    "tissue_drugs.drop(['drug_row_x','drug_row_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_drugs['num_drugs']=tissue_drugs['drugs'].apply(lambda x: len(x))\n",
    "tissue_drugs.sort_values('num_drugs', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([codes['tissue'].idx2item[tissue_id] for tissue_id in tissue_drugs['tissue']], tissue_drugs['num_drugs'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('tissue')\n",
    "plt.ylabel('number of unique drugs')\n",
    "#plt.savefig(data_path+'tissue_num_drugs.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of shared drugs between tissues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intersects=np.zeros((len(tissue_drugs),len(tissue_drugs)))\n",
    "for i, tissue_id in enumerate(tissue_drugs['tissue']):    \n",
    "    num_intersects[i,:]=tissue_drugs['drugs'].apply(lambda x: len(x.intersection(tissue_drugs.loc[tissue_id, 'drugs']))).values\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "im=ax.imshow(num_intersects)\n",
    "ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "ax.set_xticks(np.arange(len(tissue_drugs)))\n",
    "ax.set_yticks(np.arange(len(tissue_drugs)))\n",
    "ax.set_xticklabels([codes['tissue'].idx2item[tissue_id] for tissue_id in tissue_drugs['tissue']])\n",
    "ax.set_yticklabels([codes['tissue'].idx2item[tissue_id] for tissue_id in tissue_drugs['tissue']])\n",
    "\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "#fig.tight_layout()\n",
    "#plt.savefig(data_path+'heatmap.png', bb_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have successfully preprocessed all of the data required to train the model. In the next section, we will load this data back in, define the model architecture, and begin training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import Mapping\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='new_data'\n",
    "\n",
    "bsz=128\n",
    "cuda=True\n",
    "device=0\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "num_gene_compressed_drug=64\n",
    "num_gene_compressed_cell=128\n",
    "\n",
    "#isClassification=True #False for regression task\n",
    "syn_threshold=30\n",
    "ri_threshold=50\n",
    "\n",
    "log_interval=100\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug pair, cell, scoers\n",
    "df=pickle.load(open(f'{DATA_PATH}/summary/processed/summary_mean.p', 'rb'))\n",
    "codes=pickle.load(open(f'{DATA_PATH}/summary/processed/codes.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug's external features\n",
    "drug_features=pickle.load(open(f'{DATA_PATH}/drug/processed/drug_features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell's external features\n",
    "cell_features=pickle.load(open(f'{DATA_PATH}/cell/processed/cell_features.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_celllines= len(codes['cell'].idx2item)\n",
    "num_drugs=len(codes['drugs'].idx2item)\n",
    "\n",
    "num_genes = len(codes['gene'].idx2item)\n",
    "num_tissue = len(codes['tissue'].idx2item)\n",
    "num_disease = len(codes['disease'].idx2item)\n",
    "\n",
    "num_drug_fp=len(drug_features.loc[0,'fps'])\n",
    "max_drug_sm_len = drug_features['smiles'].apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gene compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugTargetDataset(Dataset):\n",
    "    def __init__(self, drug_features):\n",
    "        self.drug_features = drug_features\n",
    "    def __len__(self):\n",
    "        return len(self.drug_features)\n",
    "    def __getitem__(self,idx):\n",
    "        gene_ids=self.drug_features.loc[idx, 'gene_id']\n",
    "        genes=np.zeros(num_genes)\n",
    "        genes[gene_ids]=1\n",
    "        \n",
    "        return genes\n",
    "    \n",
    "class CellGeneDataset(Dataset):\n",
    "    def __init__(self, cell_features):\n",
    "        self.cell_features = cell_features\n",
    "    def __len__(self):\n",
    "        return len(self.cell_features)\n",
    "    def __getitem__(self,idx):\n",
    "        gene_ids=self.cell_features.loc[idx,'gene_id']\n",
    "        genes = np.zeros(num_genes)\n",
    "        for key,value in gene_ids.items():\n",
    "            genes[key]=value\n",
    "            \n",
    "        return genes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two layers of fully connected layers\n",
    "class FC2(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout):\n",
    "        super(FC2, self).__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(in_features)\n",
    "        self.fc1 = nn.Linear(in_features, int(in_features/2))\n",
    "        self.fc2 = nn.Linear(int(in_features/2),out_features)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compress gene features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneCompressor(nn.Module):\n",
    "    def __init__(self, num_in, num_out, dropout=0.1):\n",
    "        super(GeneCompressor, self).__init__()\n",
    "        self.dropout=dropout\n",
    "        self.encoder=nn.Linear(num_in, num_out)\n",
    "        self.decoder=nn.Linear(num_out,num_in)\n",
    "\n",
    "    def _encoder(self,x):\n",
    "        return F.dropout(F.relu(self.encoder(x)), self.dropout, training=self.training)\n",
    "    \n",
    "    def _decoder(self,x):\n",
    "        return F.dropout(self.decoder(x), self.dropout, training=self.training)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self._encoder(x)\n",
    "        x=self._decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geneCompressing(data_loader,num_gene_compressed, noise_weight=0.2, epochs=20, log_interval=10 ):\n",
    "    #model\n",
    "    geneCompressor=GeneCompressor(num_genes, num_out=num_gene_compressed, dropout=0.1)\n",
    "    if cuda: \n",
    "        geneCompressor=geneCompressor.cuda()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(geneCompressor.parameters())\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "        #train\n",
    "        geneCompressor.train()\n",
    "        total_loss=0\n",
    "        start_time=time.time()\n",
    "        for iteration, gene in enumerate(data_loader):\n",
    "            gene=Variable(gene).float()\n",
    "            noise=noise_weight*torch.randn(gene.shape)\n",
    "\n",
    "            if cuda:\n",
    "                gene=gene.cuda()\n",
    "                noise=noise.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output=geneCompressor(gene+noise)\n",
    "            loss=criterion(output,gene)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "            if iteration % log_interval == 0 and iteration > 0:\n",
    "                cur_loss = total_loss.item() / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(data_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "#         #test\n",
    "#         geneCompressor.eval()\n",
    "#         total_loss=0\n",
    "#         start_time=time.time()\n",
    "#         with torch.no_grad():\n",
    "#             for iteration, gene in enumerate(test_data_loader):\n",
    "#                 gene=Variable(gene).float()\n",
    "#                 if cuda:\n",
    "#                     gene=gene.cuda(device)\n",
    "#                 output=geneCompressor(gene)\n",
    "#                 loss=criterion(output,gene)\n",
    "#                 total_loss += loss.data\n",
    "#             print(total_loss.item()/iteration)\n",
    "    return geneCompressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drug's target gene data\n",
    "drugGeneDataset=DrugTargetDataset(drug_features)\n",
    "drugGeneDataset_loader = DataLoader(drugGeneDataset, batch_size=64, shuffle=True)\n",
    "#learn\n",
    "drugGeneCompressor=geneCompressing(drugGeneDataset_loader, num_gene_compressed_drug)\n",
    "#save\n",
    "drugGeneCompressor.eval()\n",
    "drugGeneCompressed=np.array([drugGeneCompressor.cpu()._encoder(torch.FloatTensor(drugGeneDataset[d])).data.numpy() for d in range(num_drugs)])\n",
    "torch.save(drugGeneCompressor.state_dict(), f'{DATA_PATH}/models/drugGeneCompressor.p')\n",
    "pickle.dump(drugGeneCompressed, open(f'{DATA_PATH}/models/drugGeneCompressed.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugGeneCompressed=pickle.load(open(f'{DATA_PATH}/models/drugGeneCompressed.p', 'rb'))\n",
    "drugGeneCompressed=torch.FloatTensor(drugGeneCompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cell line's gene expression data\n",
    "cellGeneDataset=CellGeneDataset(cell_features)\n",
    "cellGeneDataset_loader = DataLoader(cellGeneDataset, batch_size=64, shuffle=True)\n",
    "#learn\n",
    "cellGeneCompressor=geneCompressing(cellGeneDataset_loader,num_gene_compressed_cell, noise_weight=0.01, log_interval=1 )\n",
    "#save\n",
    "cellGeneCompressor.eval()\n",
    "cellGeneCompressed=np.array([cellGeneCompressor.cpu()._encoder(torch.FloatTensor(cellGeneDataset[d])).data.numpy() for d in range(num_celllines)])\n",
    "torch.save(cellGeneCompressor.state_dict(), f'{DATA_PATH}/models/cellGeneCompressor.p')\n",
    "pickle.dump(cellGeneCompressed, open(f'{DATA_PATH}/models/cellGeneCompressed.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellGeneCompressed=pickle.load(open(f'{DATA_PATH}/models/cellGeneCompressed.p', 'rb'))\n",
    "cellGeneCompressed=torch.FloatTensor(cellGeneCompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synergy prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/test split in cross or external validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_of_interest(tissues):\n",
    "    tissues_of_interests = [codes['tissue'].item2idx[minor_tissue] for minor_tissue in tissues]\n",
    "    cell_of_interest = cell_features.index[cell_features['tissue_id'].isin(tissues_of_interests)].tolist()\n",
    "    return cell_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor_tissues=['bone', 'prostate' ]\n",
    "cell_of_interest = get_cell_of_interest(minor_tissues)\n",
    "df_tissue_of_interest = df.loc[df['cell_line_name'].isin(cell_of_interest),:]\n",
    "df_all = df.drop(df_tissue_of_interest.index)\n",
    "#specific database\n",
    "#df_all=df_all.loc[df_all['study_id']==3]\n",
    "#cross validation\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.2) #cross validation\n",
    "#external validation\n",
    "#df_train=df_all.loc[df_all['study_id']==3] 3: 'ALMANAC'\n",
    "#df_test=df_all.loc[df_all['study_id']==1] 1: 'ONEIL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_bone = df.loc[df['cell_line_name'].isin(get_cell_of_interest(['bone'])),:]\n",
    "# Cross validation\n",
    "_df_train_bone, _df_test_bone = train_test_split(_df_bone, test_size=0.2, random_state=1)\n",
    "# External validation\n",
    "#_df_train_bone=_df_bone.loc[_df_bone['study_id']!=9]\n",
    "#_df_test_bone= _df_bone.loc[_df_bone['study_id']==9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test for prostate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_prostate= df.loc[df['cell_line_name'].isin(get_cell_of_interest(['prostate'])),:]\n",
    "#Cross validation\n",
    "_df_train_prostate, _df_test_prostate = train_test_split(_df_prostate, test_size=0.2, random_state=1)\n",
    "# External validation\n",
    "#_df_train_prostate=_df_prostate.loc[_df_prostate['study_id']!=1]\n",
    "#_df_test_prostate=_df_prostate.loc[_df_prostate['study_id']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugCombDataset(Dataset):\n",
    "    def __init__(self, df, drug_features, cell_features):\n",
    "        self.df = df\n",
    "        self.drug_features = drug_features\n",
    "        self.cell_features = cell_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        d1 = self.df.iloc[idx, 0]\n",
    "        d2 = self.df.iloc[idx, 1]\n",
    "        cell = self.df.iloc[idx,2]\n",
    "        ri_d1 = 1.0 if self.df.iloc[idx,3] >ri_threshold else 0\n",
    "        ri_d2 = 1.0 if self.df.iloc[idx,4] >ri_threshold else 0\n",
    "        syn = 1.0 if self.df.iloc[idx, 5] >syn_threshold else 0\n",
    "        \n",
    "        \n",
    "        #external features\n",
    "        d1_fp = np.array(self.drug_features.loc[d1, 'fps'])\n",
    "        d1_sm = self.drug_features.loc[d1, 'smiles']\n",
    "        d1_sm = np.pad(d1_sm, pad_width=(0, max_drug_sm_len-len(d1_sm)), mode='constant', constant_values=0)\n",
    "        d1_gn=drugGeneCompressed[d1]\n",
    "        \n",
    "        d2_fp = np.array(self.drug_features.loc[d2, 'fps'])\n",
    "        d2_sm = self.drug_features.loc[d2, 'smiles']\n",
    "        d2_sm = np.pad(d2_sm, pad_width=(0, max_drug_sm_len-len(d2_sm)), mode='constant', constant_values=0)\n",
    "        d2_gn=drugGeneCompressed[d2]\n",
    "        \n",
    "        c_ts = self.cell_features.loc[cell, 'tissue_id']\n",
    "        c_ds = self.cell_features.loc[cell, 'disease_id']\n",
    "        c_gn= cellGeneCompressed[cell]\n",
    "        \n",
    "        sample = {\n",
    "            'd1': d1,\n",
    "            'd1_fp': d1_fp,\n",
    "            'd1_sm': d1_sm,\n",
    "            'd1_gn': d1_gn,\n",
    "            \n",
    "            'd2': d2,\n",
    "            'd2_fp': d2_fp,\n",
    "            'd2_sm': d2_sm,\n",
    "            'd2_gn': d2_gn,\n",
    "            \n",
    "            'cell': cell,\n",
    "            'c_ts': c_ts,\n",
    "            'c_ds': c_ds, #missing -1\n",
    "            'c_gn': c_gn,\n",
    "            \n",
    "            'ri_d1': ri_d1,\n",
    "            'ri_d2': ri_d2,\n",
    "            'syn': syn\n",
    "        }\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DrugCombDataset(df_train, drug_features, cell_features)\n",
    "train_loader = DataLoader(train, batch_size=bsz, shuffle=True )\n",
    "test = DrugCombDataset(df_test, drug_features, cell_features)\n",
    "test_loader = DataLoader(test, batch_size=bsz, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_bone = DrugCombDataset(_df_train_bone, drug_features, cell_features)\n",
    "_train_loader_bone = DataLoader(_train_bone, batch_size=bsz, shuffle=True )\n",
    "_test_bone = DrugCombDataset(_df_test_bone, drug_features, cell_features)\n",
    "_test_loader_bone = DataLoader(_test_bone, batch_size=bsz, shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_prostate = DrugCombDataset(_df_train_prostate, drug_features, cell_features)\n",
    "_train_loader_prostate = DataLoader(_train_prostate, batch_size=bsz, shuffle=True )\n",
    "_test_prostate = DrugCombDataset(_df_test_prostate, drug_features, cell_features)\n",
    "_test_loader_prostate = DataLoader(_test_prostate, batch_size=bsz, shuffle=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_drugs=num_drugs,\n",
    "                 num_ID_emb=0,\n",
    "                 num_drug_fp=num_drug_fp,\n",
    "                 max_drug_sm_len=max_drug_sm_len,\n",
    "                 num_gene = num_gene_compressed_drug,\n",
    "                 num_comp_char=len(codes['mole'].idx2item),\n",
    "                 fp_embed_sz = 32,\n",
    "                 gene_embed_sz = int(num_gene_compressed_drug/2),\n",
    "                 out_size=64,\n",
    "                 dropout=0.3):\n",
    "        super(DrugEncoder, self).__init__()\n",
    "        \n",
    "        self.dropout= dropout\n",
    "        #DRUG\n",
    "        #drug ID\n",
    "        #self.embed_id = nn.Embedding(num_drugs, num_ID_emb)\n",
    "        \n",
    "        #compound ID\n",
    "        self.embed_comp = nn.Embedding(num_comp_char, num_comp_char, padding_idx=0)#padding's idx=0\n",
    "        #encoding compound\n",
    "        self.encoderlayer = nn.TransformerEncoderLayer(d_model=num_comp_char, nhead=4)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderlayer, num_layers=1)\n",
    "        \n",
    "        #fingerprint\n",
    "        self.dense_fp = nn.Linear(num_drug_fp,fp_embed_sz)\n",
    "        #gene\n",
    "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
    "        \n",
    "        #depthwise for compound encoding\n",
    "        self.conv = nn.Conv2d(1, 1, (1, num_comp_char), groups=1)\n",
    "        \n",
    "        #combined\n",
    "        combined_sz = num_ID_emb+fp_embed_sz+max_drug_sm_len+gene_embed_sz\n",
    "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
    "\n",
    "    def forward(self, d_list):\n",
    "        \"\"\"\n",
    "            id: bsz*1\n",
    "            fp: bsz*num_drug_fp\n",
    "            sm: bsz*max_drug_sm_len\n",
    "        \"\"\"\n",
    "        id, fp, sm, gn = d_list\n",
    "        \n",
    "        sm = self.embed_comp(sm) #bsz*max_drug_sm_len*num_comp_char(embedding size)\n",
    "        sm = self.encoder(sm)\n",
    "        sm = self.conv(sm.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        fp = F.relu(self.dense_fp(fp))\n",
    "        gn = F.relu(self.dense_gene(gn))\n",
    "        \n",
    "        #combine\n",
    "        x = torch.cat((fp, sm, gn),1) # bsz*[num_emb_id+num_drug_fp+max+drug_sm]\n",
    "        x = self.FC2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_cells=num_celllines,\n",
    "                 num_tissue=0,\n",
    "                 num_disease=num_disease,\n",
    "                 num_ID_emb=0,\n",
    "                 gene_embed_sz=int(num_gene_compressed_cell/2),\n",
    "                 num_gene=num_gene_compressed_cell,\n",
    "                 out_size=64,\n",
    "                 dropout=0.3):\n",
    "        super(CellEncoder, self).__init__()\n",
    "        \n",
    "        self.dropout= dropout\n",
    "        #cell ID\n",
    "        #self.embed_id = nn.Embedding(num_cells, num_ID_emb)\n",
    "        #cell tissue\n",
    "        #self.embed_ts = nn.Embedding(num_tissue, num_tissue)\n",
    "        #cell disease\n",
    "        self.embed_ds = nn.Embedding(num_disease, num_disease, padding_idx=3)\n",
    "        #gene\n",
    "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
    "        \n",
    "        #combined\n",
    "        combined_sz = num_ID_emb+num_tissue+num_disease+gene_embed_sz\n",
    "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
    "    \n",
    "        \n",
    "    def forward(self, c_list):\n",
    "        \"\"\"\n",
    "            id: bsz*1\n",
    "            fp: bsz*num_drug_fp\n",
    "            sm: bsz*max_drug_sm_len\n",
    "        \"\"\"\n",
    "        id, ts, ds, gn = c_list\n",
    "        ds = F.relu(self.embed_ds(ds)) #bsz*num_diesaes\n",
    "        \n",
    "        gn = F.relu(self.dense_gene(gn)) #bsz*gene_embed_sz\n",
    "        \n",
    "        #combine\n",
    "        x = torch.cat((ds, gn),1) # bsz*combined_sz\n",
    "        x = self.FC2(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comb(nn.Module):\n",
    "    def __init__(self, num_cells=num_celllines, \n",
    "                 num_drugs=num_drugs,\n",
    "                 num_drug_fp=num_drug_fp,\n",
    "                 max_drug_sm_len=max_drug_sm_len,\n",
    "                num_comp_char=len(codes['mole'].idx2item),\n",
    "                 num_ID_emb=0,\n",
    "                 out_size=64,\n",
    "                dropout=0.3):\n",
    "        \n",
    "        super(Comb, self).__init__()\n",
    "        \n",
    "        self.dropout=dropout    \n",
    "        #drug\n",
    "        self.drugEncoder = DrugEncoder()\n",
    "        #cell\n",
    "        self.cellEncoder = CellEncoder()\n",
    "        #fc\n",
    "        self.fc_syn = FC2(out_size*3, 1, dropout)\n",
    "        self.fc_ri = FC2(out_size*2, 1, dropout)\n",
    "        \n",
    "    def forward(self, d1_list, d2_list, c_list):\n",
    "        d1 = self.drugEncoder(d1_list)\n",
    "        d2 = self.drugEncoder(d2_list)\n",
    "        c = self.cellEncoder(c_list)\n",
    "        \n",
    "        syn = self.fc_syn(torch.cat((d1, d2, c),1))\n",
    "        ri1 = self.fc_ri(torch.cat((d1,c),1))\n",
    "        ri2 = self.fc_ri(torch.cat((d2,c),1))\n",
    "        \n",
    "        return syn, ri1, ri2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Comb()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "#Regression\n",
    "#criterion_mse = nn.MSELoss()\n",
    "#Classification\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adagrad(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "def training(isAux, data_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for iteration, sample in enumerate(data_loader):\n",
    "        d1=Variable(sample['d1'])\n",
    "        d1_fp = Variable(sample['d1_fp'].float())\n",
    "        d1_sm = Variable(sample['d1_sm'])\n",
    "        d1_gn = Variable(sample['d1_gn'].float())\n",
    "        \n",
    "        d2=Variable(sample['d2'])\n",
    "        d2_fp = Variable(sample['d2_fp'].float())\n",
    "        d2_sm = Variable(sample['d2_sm'])\n",
    "        d2_gn = Variable(sample['d2_gn'].float())\n",
    "        \n",
    "        cell = Variable(sample['cell'])\n",
    "        c_ts = Variable(sample['c_ts'])\n",
    "        c_ds = Variable(sample['c_ds'])\n",
    "        c_gn = Variable(sample['c_gn'].float())\n",
    "        \n",
    "        syn_true = Variable(sample['syn'].float())\n",
    "        ri_d1=Variable(sample['ri_d1'].float())\n",
    "        ri_d2=Variable(sample['ri_d2'].float())\n",
    "\n",
    "\n",
    "        if cuda:\n",
    "            d1=d1.cuda()\n",
    "            d1_fp=d1_fp.cuda()\n",
    "            d1_sm=d1_sm.cuda()\n",
    "            d1_gn=d1_gn.cuda()\n",
    "            \n",
    "            d2=d2.cuda()\n",
    "            d2_fp=d2_fp.cuda()\n",
    "            d2_sm=d2_sm.cuda()\n",
    "            d2_gn=d2_gn.cuda()\n",
    "            \n",
    "            cell=cell.cuda()\n",
    "            c_ts=c_ts.cuda()\n",
    "            c_ds=c_ds.cuda()\n",
    "            c_gn=c_gn.cuda()\n",
    "            \n",
    "            syn_true=syn_true.cuda()\n",
    "            ri_d1=ri_d1.cuda()\n",
    "            ri_d2=ri_d2.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        syn, ri1, ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts, c_ds, c_gn) )\n",
    "        \n",
    "        if not isAux:\n",
    "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
    "        else:\n",
    "            loss = criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if iteration % log_interval == 0 and iteration > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(train_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
    "\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_sen = 0\n",
    "\n",
    "    \n",
    "    #loss\n",
    "    with torch.no_grad():\n",
    "        for iteration, sample in enumerate(data_loader):\n",
    "            d1=Variable(sample['d1'])\n",
    "            d1_fp = Variable(sample['d1_fp'].float())\n",
    "            d1_sm = Variable(sample['d1_sm'])\n",
    "            d1_gn = Variable(sample['d1_gn'].float())\n",
    "\n",
    "            d2=Variable(sample['d2'])\n",
    "            d2_fp = Variable(sample['d2_fp'].float())\n",
    "            d2_sm = Variable(sample['d2_sm'])\n",
    "            d2_gn = Variable(sample['d2_gn'].float())\n",
    "\n",
    "            cell = Variable(sample['cell'])\n",
    "            c_ts = Variable(sample['c_ts'])\n",
    "            c_ds = Variable(sample['c_ds'])\n",
    "            c_gn = Variable(sample['c_gn'].float())\n",
    "\n",
    "            syn_true = Variable(sample['syn'].float())\n",
    "            ri_d1=Variable(sample['ri_d1'].float())\n",
    "            ri_d2=Variable(sample['ri_d2'].float())\n",
    "\n",
    "\n",
    "            if cuda:\n",
    "                d1=d1.cuda()\n",
    "                d1_fp=d1_fp.cuda()\n",
    "                d1_sm=d1_sm.cuda()\n",
    "                d1_gn=d1_gn.cuda()\n",
    "\n",
    "                d2=d2.cuda()\n",
    "                d2_fp=d2_fp.cuda()\n",
    "                d2_sm=d2_sm.cuda()\n",
    "                d2_gn=d2_gn.cuda()\n",
    "\n",
    "                cell=cell.cuda()\n",
    "                c_ts=c_ts.cuda()\n",
    "                c_ds=c_ds.cuda()\n",
    "                c_gn=c_gn.cuda()\n",
    "\n",
    "                syn_true=syn_true.cuda()\n",
    "                ri_d1=ri_d1.cuda()\n",
    "                ri_d2=ri_d2.cuda()\n",
    "\n",
    "\n",
    "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
    "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
    "            total_loss +=loss.data\n",
    "            loss_sen = (criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1)))/2\n",
    "            total_loss_sen += loss_sen.data\n",
    "\n",
    "        print('syn mse', total_loss.item()/(iteration+1))\n",
    "        print('sen_mse', total_loss_sen.item()/(iteration+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        training(False, train_loader)\n",
    "        training(True, train_loader)\n",
    "        evaluate(test_loader)\n",
    "        print('-'*89)\n",
    "except KeyboardInterrupt:\n",
    "    print('-'*89)\n",
    "    print('Existing from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'id_feat_gene_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model save\n",
    "torch.save(model.state_dict(), f'{DATA_PATH}/models/{model_name}.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model load\n",
    "model.load_state_dict(torch.load(f'{DATA_PATH}/models/{model_name}.p'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(y_score, y_true, k):\n",
    "    \"\"\"\n",
    "        https://www.kaggle.com/davidgasquez/ndcg-scorer\n",
    "        y_true: np.array, size= [n_samples]\n",
    "        y_score: np.array, size=[n_samples]\n",
    "        k: int, rank\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "    #gain = 2 ** y_true -1\n",
    "    gain = y_true \n",
    "    \n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain/discounts)\n",
    "\n",
    "def evaluate_accuracy(data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    syn_all=[]\n",
    "    syn_true_all=[]\n",
    "    ri1_all=[]\n",
    "    ri1_true_all=[]\n",
    "    ri2_all=[]\n",
    "    ri2_true_all=[]\n",
    "    \n",
    "    #loss\n",
    "    with torch.no_grad():\n",
    "        for iteration, sample in enumerate(data_loader):\n",
    "            d1=Variable(sample['d1'])\n",
    "            d1_fp = Variable(sample['d1_fp'].float())\n",
    "            d1_sm = Variable(sample['d1_sm'])\n",
    "            d1_gn = Variable(sample['d1_gn'].float())\n",
    "\n",
    "            d2=Variable(sample['d2'])\n",
    "            d2_fp = Variable(sample['d2_fp'].float())\n",
    "            d2_sm = Variable(sample['d2_sm'])\n",
    "            d2_gn = Variable(sample['d2_gn'].float())\n",
    "\n",
    "            cell = Variable(sample['cell'])\n",
    "            c_ts = Variable(sample['c_ts'])\n",
    "            c_ds = Variable(sample['c_ds'])\n",
    "            c_gn = Variable(sample['c_gn'].float())\n",
    "\n",
    "            syn_true = Variable(sample['syn'].float())\n",
    "            ri_d1=Variable(sample['ri_d1'])\n",
    "            ri_d2=Variable(sample['ri_d2'])\n",
    "\n",
    "\n",
    "            if cuda:\n",
    "                d1=d1.cuda()\n",
    "                d1_fp=d1_fp.cuda()\n",
    "                d1_sm=d1_sm.cuda()\n",
    "                d1_gn=d1_gn.cuda()\n",
    "\n",
    "                d2=d2.cuda()\n",
    "                d2_fp=d2_fp.cuda()\n",
    "                d2_sm=d2_sm.cuda()\n",
    "                d2_gn=d2_gn.cuda()\n",
    "\n",
    "                cell=cell.cuda()\n",
    "                c_ts=c_ts.cuda()\n",
    "                c_ds=c_ds.cuda()\n",
    "                c_gn=c_gn.cuda()\n",
    "\n",
    "\n",
    "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
    "            \n",
    "            syn_all.append(syn.data.cpu().numpy())\n",
    "            syn_true_all.append(syn_true.numpy())\n",
    "            \n",
    "            ri1_all.append(ri1.data.cpu().numpy())\n",
    "            ri1_true_all.append(ri_d1.numpy())\n",
    "            \n",
    "            ri2_all.append(ri2.data.cpu().numpy())\n",
    "            ri2_true_all.append(ri_d2.numpy())\n",
    "            \n",
    "    return syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate synergy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(test_loader)\n",
    "\n",
    "syn_all= [s.item() for syn in syn_all for s in syn]\n",
    "syn_true_all = [s for syn in syn_true_all for s in syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUPRC\n",
    "metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUROC\n",
    "metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate sensitivity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
    "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
    "\n",
    "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
    "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
    "\n",
    "ri_all=ri1_all+ri2_all\n",
    "ri_true_all=ri1_true_all+ri2_true_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer the general model to specific model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to boost a bit more with general model's test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use major's test set\n",
    "training(False, test_loader)\n",
    "training(True, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the layer's ID that we'd like to fix or free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    print(i, param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_after = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    if i>=release_after:\n",
    "        param.requires_grad=True\n",
    "    else:\n",
    "        param.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prostate or bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _train_loader_minor=_train_loader_prostate\n",
    "# _test_loader_minor=_test_loader_prostate\n",
    "# _test_minor=_test_prostate\n",
    "_train_loader_minor=_train_loader_bone\n",
    "_test_loader_minor=_test_loader_bone\n",
    "_test_minor=_test_bone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use minor's train set\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        training(False, _train_loader_minor)\n",
    "        training(True, _train_loader_minor)\n",
    "        evaluate(_test_loader_minor)\n",
    "        print('-'*89)\n",
    "except KeyboardInterrupt:\n",
    "    print('-'*89)\n",
    "    print('Existing from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate synergy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(_test_loader_minor)\n",
    "\n",
    "syn_all= [s.item() for syn in syn_all for s in syn]\n",
    "syn_true_all = [s for syn in syn_true_all for s in syn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUROC\n",
    "metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUPRC\n",
    "metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate sensitivity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
    "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
    "\n",
    "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
    "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
    "\n",
    "ri_all=ri1_all+ri2_all\n",
    "ri_true_all=ri1_true_all+ri2_true_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NDCG\n",
    "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the top ranked drug combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_all_prob=1/(1 + np.exp(-np.array(syn_all)))\n",
    "order = np.argsort(syn_all_prob)[::-1]\n",
    "syn_true_all_order = np.take(syn_true_all, order[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    comb=_test_minor[order[k]]\n",
    "    print(codes['drugs'].idx2item[comb['d1']], ',', \n",
    "          codes['drugs'].idx2item[comb['d2']], ',', \n",
    "          codes['cell'].idx2item[comb['cell']],  ',', \n",
    "          codes['tissue'].idx2item[comb['c_ts']],  ',',\n",
    "          codes['disease'].idx2item[comb['c_ds']], ',', \n",
    "          comb['ri_d1'], ',',\n",
    "          comb['ri_d2'], ',',\n",
    "          syn_true_all_order[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug response prediction accuracy for data-rich tissues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment involved training the model on a varying number of input features for both cells and drugs. I was unable to refactor the provided code sufficiently to perform the experiments without certain input features, so the results shown in the table are only for the combination of ID, SMILES, and Target Genes. Results shown in Table 1 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan='2'>Input Features</th>\n",
    "    <th colspan='2'>Monotherapy Sensitivity</th>\n",
    "    <th colspan='2'>Combination Synergy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NDCG</td>\n",
    "    <td>AUC</td>\n",
    "    <td>NDCG</td>\n",
    "    <td>AUC</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ID+F+G</td>\n",
    "    <td>0.3388</td>\n",
    "    <td>0.8651</td>\n",
    "    <td>1.0000</td>\n",
    "    <td>0.9627</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p><b>Table 1:</b> Drug response preduction accuracy for data-rich tissues</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the resuls of the original paper, my own results are a bit mixed. On one hand, the accuracy of the model predictions of combination synergy is actually slightly higher than those reported by [Kim et al. (2020)](https://doi.org/10.1101/2020.02.05.932657) by a margin of approximately 0.08 points. Similarly, the NDCG is comparatively higher at 1.00, whereas the original authors reported an NDCG for combination synergy at 0.8377. These seemingly positive results become more perplexing when we consider the monotherapy sensitivity scores, which are much lower for NDCG and AUC by .0847 and 0.6612 points, respectively. Given that I followed the same approach and used their own architecture for the models themselves, these results are striking. Namely the sensitivity score of 0.3388 for NDCG. The only plausible explanation for the discrepancy leads back to the issue with data. Without any guarantee that the restructuring of the new data to align with the dimensionality of the original data was without errors, it's possible that some key data present in the original models was absent in my own analysis. Perhaps even more confusing is the fact that in spite of these likely unintended data exclusions, the model performed even better at predicting the combination synergy. I can only assume that different data and/or preprocessing step(s) were excluded that might offer some insight into the discrepancies between these scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug response prediction accuracy for bone and prostate cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<table>\n",
    "  <tr>\n",
    "    <th rowspan='2'>Tissue</th>\n",
    "    <th rowspan='2'>Input Features</th>\n",
    "    <th colspan='2'>Monotherapy Sensitivity</th>\n",
    "    <th colspan='2'>Combination Synergy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NDCG</td>\n",
    "    <td>AUC</td>\n",
    "    <td>NDCG</td>\n",
    "    <td>AUC</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Bone</td>\n",
    "    <td>ID+F+G</td>\n",
    "    <td>0.0000</td>\n",
    "    <td>0.7074</td>\n",
    "    <td>0.3760</td>\n",
    "    <td>0.6655</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Prostate</td>\n",
    "    <td>ID+F+G</td>\n",
    "    <td>0.0897</td>\n",
    "    <td>0.8143</td>\n",
    "    <td>0.9644</td>\n",
    "    <td>0.9690</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<p><b>Table 2:</b> Drug response preduction accuracy for bone and prostate cancer</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Predicted Synergenistic Drug Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prostate Top Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>#</th>\n",
    "      <th>Drug 1</th>\n",
    "      <th>Drug 2</th>\n",
    "      <th>Cell</th>\n",
    "      <th>Tissue</th>\n",
    "      <th>Disease</th>\n",
    "      <th>ri_d1</th>\n",
    "      <th>ri_d2</th>\n",
    "      <th>syn_true_all_order</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>AXITINIB</td>\n",
    "      <td>CABAZITAXEL</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>ACTINOMYCIN D</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>RUXOLITINIB</td>\n",
    "      <td>CABAZITAXEL</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>FLUDARABINE BASE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>VINBLASTINE SULFATE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>AMIFOSTINE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>6</td>\n",
    "      <td>IMATINIB</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7</td>\n",
    "      <td>IMATINIB</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>8</td>\n",
    "      <td>TOPOTECAN HYDROCHLORIDE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>9</td>\n",
    "      <td>HYDROXYUREA</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>10</td>\n",
    "      <td>TEMOZOLOMIDE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>11</td>\n",
    "      <td>CO-V</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>12</td>\n",
    "      <td>CHEMBL17639</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>13</td>\n",
    "      <td>PLICAMYCIN</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>14</td>\n",
    "      <td>ARSENIC TRIOXIDE</td>\n",
    "      <td>NSC256439</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>15</td>\n",
    "      <td>PRALATREXATE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>16</td>\n",
    "      <td>LETROZOLE</td>\n",
    "      <td>IXABEPILONE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>17</td>\n",
    "      <td>RUXOLITINIB</td>\n",
    "      <td>ADM HYDROCHLORIDE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>18</td>\n",
    "      <td>ACTINOMYCIN D</td>\n",
    "      <td>ARSENIC TRIOXIDE</td>\n",
    "      <td>DU-145</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>19</td>\n",
    "      <td>PAZOPANIB HYDROCHLORIDE</td>\n",
    "      <td>CABAZITAXEL</td>\n",
    "      <td>PC-3</td>\n",
    "      <td>prostate</td>\n",
    "      <td>c4863</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p><b>Table 3:</b> Top 20 synergy predictions for prostate</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bone Top Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>#</th>\n",
    "        <th>Drug 1</th>\n",
    "        <th>Drug 2</th>\n",
    "        <th>Cell</th>\n",
    "        <th>Tissue</th>\n",
    "        <th>Disease</th>\n",
    "        <th>ri_d1</th>\n",
    "        <th>ri_d2</th>\n",
    "        <th>syn_true_all_order</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>ISODONOL</td>\n",
    "        <td>PD325901</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>AK-77283</td>\n",
    "        <td>OLAPARIB</td>\n",
    "        <td>TC-71</td>\n",
    "        <td>bone</td>\n",
    "        <td>c4817</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>DASATINIB</td>\n",
    "        <td>BEZ-235</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>TRAMETINIB (GSK1120212)</td>\n",
    "        <td>CUDC-101</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>CUDC-101</td>\n",
    "        <td>PD325901</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>ZINC34894448</td>\n",
    "        <td>thapsigargin</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>ZINC34894448</td>\n",
    "        <td>BARDOXOLONE METHYL</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>117048-62-1</td>\n",
    "        <td>CARFILZOMIB (PR-171)</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>117048-62-1</td>\n",
    "        <td>Luminespib</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>ISODONOL</td>\n",
    "        <td>CUDC-101</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>AK-77283</td>\n",
    "        <td>CARFILZOMIB (PR-171)</td>\n",
    "        <td>TC-71</td>\n",
    "        <td>bone</td>\n",
    "        <td>c4817</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>11</td>\n",
    "        <td>Fingolimod</td>\n",
    "        <td>proscillaridin</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>12</td>\n",
    "        <td>CHEMBL2140543</td>\n",
    "        <td>PD325901</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>13</td>\n",
    "        <td>BEZ-235</td>\n",
    "        <td>thapsigargin</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>14</td>\n",
    "        <td>CHEMBL2140543</td>\n",
    "        <td>BARDOXOLONE METHYL</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>15</td>\n",
    "        <td>CARFILZOMIB (PR-171)</td>\n",
    "        <td>PANOBINOSTAT</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>16</td>\n",
    "        <td>117048-62-1</td>\n",
    "        <td>CUDC-101</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>17</td>\n",
    "        <td>DORSOMORPHIN</td>\n",
    "        <td>thapsigargin</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>18</td>\n",
    "        <td>BEZ-235</td>\n",
    "        <td>CUDC-101</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>19</td>\n",
    "        <td>117048-62-1</td>\n",
    "        <td>TRAMETINIB (GSK1120212)</td>\n",
    "        <td>TC-32</td>\n",
    "        <td>bone</td>\n",
    "        <td>c3716</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1.0</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<p><b>Table 4:</b>Top 20 synergy predictions for bone</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues Encountered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of my best efforts, I was unable to fully reproduce the results of [Kim et al. (2020).](https://doi.org/10.1101/2020.02.05.932657) I do not believe that this paper is reproducible in its current state, given the lack of documentation and the unavailability of data. I believe that the primary issue with this reproduction attempt was the ambiguity of where the data came from, and in what form. While I initially mistook the publicly available data as a good sign that this step would be comparatively easy, I soon found that acquiring the data originally used for this paper was a non-trivial task—as the data available in DrugComb has since been updated to a point in which there was no way to get the names of tissues and diseases aligned with their respective ids, and the data accessed from DrugBank was not documented anywhere in the original paper or in the repository. To that point, the only way I managed to access *some* of the original data was by reaching out to several of the original authors directly; even then, they did not share the complete set of original data they used, which left me struggling to refactor the code to align with the dimensionality and nuances of the combined new and original datasets. For more information on the specific challenges faced from a data perspective, please see the [Scope of Reproducability](#scope-of-reproducability) section above. \n",
    "\n",
    "The challenge of data collection proved to be multifaceted, as the scripts themselves were designed with only the original datasets in mind; this necessitated that I make significant changes throughout the code to align with synthesis of new and original data. I spent a significant amount of time attempting to refactor the code, but without a background in molecular biology and no documentation I was left struggling to interpret whether the dimensions in the new data were in fact aligned with those of the original. Given the number of variables, I was, in every case, forced to consider multiple scenarios when encountering errors running the code; these could have come from a misalignment in the dimensions of the new and original data, an oversight in my code with respect to the methodology used by the authors, or simply a nuance in the functionality of the packages due to differences in versions used. Unfortunately, the lack of documentation precluded me from being able to easily narrow down where the issues were coming from, and I was instead left spending a significant amount of time refactoring the data and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducability and Future Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate, I do not believe that this paper is reproducible in its current state. Without proper documentation or processes in place for accessing the original data used, there is no way to ensure consistency across reproduction attempts. Furthermore, repeatedly contacting the authors in the hopes of acquiring the original data has diminishing returns as they are likely busy people to the extent that a multitude of students contacting them all requesting the same thing most likely comes across as annoying. On the other hand, it behooves the authors of any academic paper to make their results reproducible and documented—and the persistent issue of requests concerning the data is one created by their own design. This is all to say that there is nuance to be considered with regard to this topic. To what extent is the inability to reproduce a paper the fault of the person attempting the reproduction or the fault of the authors? The reality is most likely somewhere in the middle. Reflecting on my own faults in this process, the late change of paper undoubtedly caused me to have a shorter runway for completing this project. It's also possible that I made a wrong turn in attempting to reuse the authors' code and data. While this approach seemed like the most logical thing to do at the time, it's true that with the time spent refactoring may have been better spent simply building a solution from the ground up. Unfortunately, that was not the avenue I pursued, and the results of this reproduction attempt speak to that.\n",
    "\n",
    "In terms of recommendations to Kim et al. for the future, I would strongly suggest they make the original data (all of it) accessible for reproduction attempts. Sharing only partials creates even more confusion in attempting to align dimensionality of new and original data, as described above. Doing this would narrow the margin of error in reproduction attempts insofaras errors in the code would be significantly easier to troubleshoot. Furthermore, this would cut down on the number of requests the authors receive concerning this, which is undoubtedly in their best interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: Kim, Y., Zheng, S., Tang, J., Zheng, W. J., Li, Z., & Jiang, X. (2020). Anti-Cancer Drug Synergy Prediction in Understudied Tissues Using Transfer Learning. https://doi.org/10.1101/2020.02.05.932657 \n",
    "\n",
    "[2]: Liu, S., Nie, W., Wang, C., Lu, J., Qiao, Z., Liu, L., Tang, J., Xiao, C., & Anandkumar, A. (2023). Multi-modal Molecule structure–text model for text-based retrieval and editing. Nature Machine Intelligence, 5(12), 1447–1457. https://doi.org/10.1038/s42256-023-00759-6\n",
    "\n",
    "[3]: Zagidullin, B., Aldahdooh, J., Zheng, S., Wang, W., Wang, Y., Saad, J., Malyutina, A., Jafari, M., Tanoli, Z., Pessia, A., & Tang, J. (2019). Drugcomb: An integrative cancer drug combination data portal. Nucleic Acids Research, 47(W1). https://doi.org/10.1093/nar/gkz337 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synergy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
